<h2> 
<a href="https://vistadream-project-page.github.io/" target="_blank">VistaDream: Sampling multiview consistent images for single-view scene reconstruction</a>
</h2>

This is the official PyTorch implementation of the following publication:

> **VistaDream: Sampling multiview consistent images for single-view scene reconstruction**<br/>
> [Haiping Wang](https://hpwang-whu.github.io/), [Yuan Liu](https://liuyuan-pal.github.io/), [Ziwei Liu](https://liuziwei7.github.io/), [Wenping Wang](https://www.cs.hku.hk/people/academic-staff/wenping), [Zhen Dong](https://dongzhenwhu.github.io/index.html), [Bisheng Yang](https://3s.whu.edu.cn/info/1025/1415.htm)<br/>
> *ICCV 2025*<br/>
> [**Paper**](https://arxiv.org/abs/2410.16892) | [**Project-page**(with Interactive DEMOs)](https://vistadream-project-page.github.io/) 


## ğŸ”­ Introduction
<p align="center">
<strong>TL;DR: VistaDream is a training-free framework to reconstruct a high-quality 3D scene from a single-view image.</strong>
</p>

<table style="width:100%;">
  <tr>
    <td style="text-align:center; vertical-align:middle;">
      <img src="data/assert/victorian.rgb.png" alt="Image" style="height:200px;">
    </td>
    <td style="text-align:center; vertical-align:middle;">
      <img src="data/assert/victorian.rgb.gif" alt="RGB GIF" style="height:200px;">
    </td>
    <td style="text-align:center; vertical-align:middle;">
      <img src="data/assert/victorian.dpt.gif" alt="Depth GIF" style="height:200px;">
    </td>
  </tr>
  <tr>
    <td style="text-align:center;">Input Image</td>
    <td style="text-align:center;">RGBs of the reconstructed scene</td>
    <td style="text-align:center;">Depths of the reconstructed scene</td>
  </tr>
  <tr>
    <td colspan="3" style="text-align:center;">
      More results and interactive demos are provided in the 
      <a href="https://vistadream-project-page.github.io/" target="_blank">Project Page</a>.
    </td>
  </tr>
</table>

<p align="justify">
  æ‘˜è¦ï¼š æœ¬æ–‡æå‡ºäº† VistaDreamï¼Œä¸€ç§ä»å•è§†å›¾å›¾åƒé‡å»º 3D åœºæ™¯çš„æ–°å‹æ¡†æ¶ã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»å•è§†å›¾è¾“å…¥å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»…ä¸“æ³¨äºæ„å»ºè¾“å…¥å›¾åƒä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå´å¿½ç•¥äº†ç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚
VistaDream é€šè¿‡ä¸¤é˜¶æ®µæµæ°´çº¿è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼š
  * ç¬¬ä¸€é˜¶æ®µï¼šVistaDream é¦–å…ˆé€šè¿‡å‘å¤–æ‰©å±•è¾¹ç•Œå¹¶ç»“åˆä¼°è®¡çš„æ·±åº¦å›¾ï¼Œæ„å»ºä¸€ä¸ªå…¨å±€ç²—ç²’åº¦çš„ 3D æ¡†æ¶ã€‚ç„¶åï¼Œåœ¨è¿™ä¸ªå…¨å±€æ¡†æ¶ä¸Šï¼Œåˆ©ç”¨åŸºäºæ‰©æ•£çš„è¿­ä»£ RGB-D ä¿®å¤æŠ€æœ¯ç”Ÿæˆæ–°è§†å›¾å›¾åƒï¼Œä»¥å¡«å……æ¡†æ¶ä¸­çš„ç©ºæ´ã€‚
  * ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹å¤šè§†å›¾ä¸€è‡´æ€§é‡‡æ ·ï¼ˆMCSï¼‰æ–¹æ³•ï¼Œåœ¨æ‰©æ•£æ¨¡å‹çš„åå‘é‡‡æ ·è¿‡ç¨‹ä¸­å¼•å…¥å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸï¼Œè¿›ä¸€æ­¥å¢å¼ºç”Ÿæˆçš„æ–°è§†å›¾å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚
å®éªŒç»“æœè¡¨æ˜ï¼ŒVistaDream æ— éœ€è®­ç»ƒæˆ–å¾®è°ƒç°æœ‰æ‰©æ•£æ¨¡å‹ï¼Œä»…ä½¿ç”¨å•è§†å›¾å›¾åƒå³å¯å®ç°ä¸€è‡´ä¸”é«˜è´¨é‡çš„æ–°è§†å›¾åˆæˆï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

</p>

## ğŸ†• News
- 2025-06-26: VistaDream is accepted by ICCV 2025!
- 2024-10-23: Code, [[project page]](https://vistadream-project-page.github.io/), and [[arXiv paper]](https://arxiv.org/abs/2410.16892) are aviliable.

## ğŸ’» Requirements
The code has been tested on:
- Ubuntu 20.04
- CUDA 12.3
- Python 3.10.12
- Pytorch 2.1.0
- GeForce RTX 4090.

## ğŸ”§ Installation
For complete installation instructions, please see [INSTALL.md](INSTALL.md).

## ğŸš… Pretrained model
VistaDream is training-free but utilizes pretrained models of several existing projects.
To download pretrained models for [Fooocus](https://github.com/lllyasviel/Fooocus), [Depth-Pro](https://github.com/apple/ml-depth-pro), 
[OneFormer](https://github.com/SHI-Labs/OneFormer), [SD-LCM](https://github.com/luosiallen/latent-consistency-model), run the following command:
```
bash download_weights.sh
```
The pretrained models of [LLaVA](https://github.com/haotian-liu/LLaVA) and [Stable Diffusion-1.5](https://github.com/CompVis/stable-diffusion) will be automatically downloaded from hugging face on the first running.

## ğŸ”¦ Demo (Single-View Generation)
Try VistaDream using the following commands:
```
python demo.py
```
Then, you should obtain:
- ```data/sd_readingroom/scene.pth```: the generated gaussian field;
- ```data/sd_readingroom/video_rgb(dpt).mp4```: the rgb(dpt) renderings from the scene.

## ğŸ”¦ Demo (Sparse-View Generation)
To use sparse views as input as [demo_here](https://github.com/WHU-USI3DV/VistaDream/issues/14), we need [Dust3r](https://github.com/naver/dust3r) to first reconstruct the input images to 3D as the scaffold (no zoom-out needed).

First download Dust3r [checkpoints](https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth) and place it at ```tools/Dust3r/checkpoints``` by the following command:
```
wget -P tools/Dust3r/checkpoints https://download.europe.naverlabs.com/ComputerVision/DUSt3R/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth
```
Then try VistaDream with sparse images as input using the following commands:
```
python demo_sparse.py
```
Then, you should obtain:
- ```data/bedroom/scene.pth```: the generated gaussian field;
- ```data/bedroom/video_rgb(dpt).mp4```: the rgb(dpt) renderings from the scene.

## ğŸ”¦ Generate your own scene (Single or Sparse views as input)
If you need to improve the reconstruction quality of your own images, please refer to [INSTRUCT.md](pipe/cfgs/INSTRUCT.md)

To visualize the generated gaussian field, you can use the following script:
```
import torch
from ops.utils import save_ply
scene = torch.load(f'data/vistadream/piano/refine.scene.pth')
save_ply(scene,'gf.ply')
```
and feed the ```gf.ply``` to [SuperSplat](https://playcanvas.com/supersplat/editor) for visualization.

## ğŸ”¦ ToDo List
- [x] Early check in generation.
- [x] Support more types of camera trajectory. Please follow [Here](ops/trajs/TRAJECTORY.MD) to define your trajectory. An example is given in this [issue](https://github.com/WHU-USI3DV/VistaDream/issues/11).
- [x] Support sparse-view-input (and no pose needed). An example is given in this [issue](https://github.com/WHU-USI3DV/VistaDream/issues/14).
- [ ] Interactive Demo.

## ğŸ”— Related Projects
We sincerely thank the excellent open-source projects:
- [Fooocus](https://github.com/lllyasviel/Fooocus) for the wonderful inpainting quality;
- [LLaVA](https://github.com/haotian-liu/LLaVA) for the wonderful image analysis and QA ability;
- [Depth-Pro](https://github.com/apple/ml-depth-pro) for the wonderful monocular metric depth estimation accuracy;
- [OneFormer](https://github.com/SHI-Labs/OneFormer) for the wonderful sky segmentation accuracy;
- [StableDiffusion](https://github.com/CompVis/stable-diffusion) for the wonderful image generation/optimization ability.
